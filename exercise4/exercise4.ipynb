{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1 Neural Networks\n",
    "In this exercise, we would implement a one-hidden-layer neural network using backward propagation method.\n",
    "\n",
    "### Steps\n",
    "1. Randomly initialize the weights\n",
    "2. Implement forward propagation to get $h_\\Theta(x^{(i)})$ for any $x^{(i)}$\n",
    "3. Implement the cost function\n",
    "4. Implement backpropagation to compute partial derivatives\n",
    "5. Use gradient checking to confirm that your backpropagation works. Then disable gradient checking.\n",
    "6. Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta.\n",
    "\n",
    "## 1.1 Visualizing the data\n",
    "This part is the same as what we did in exercise3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using MAT\n",
    "using Images, Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "restore (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data from the .mat files\n",
    "data = matread(\"data/ex4data1.mat\")\n",
    "images, numbers = data[\"X\"], data[\"y\"]\n",
    "\n",
    "# The width of the small image\n",
    "const WIDTH = convert(Int64, sqrt(length(images[1, :])))\n",
    "\n",
    "# Reshape one row of the training example into a square matrix\n",
    "function restore(row::Array{Float64, 1})\n",
    "    width = convert(Int64,sqrt(length(row)))\n",
    "    return  clamp01nan.(reshape(row, width, width))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "move_image! (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move one small image matrix into a bigger matrix at given position\n",
    "function move_image!(row::Int64, col::Int64, square::Array{Float64, 2}, \n",
    "                    im::Array{Float64, 2})\n",
    "    for i in 0:WIDTH-1\n",
    "        for j in 0:WIDTH-1\n",
    "            square[row + i, col + j] = im[i + 1, j + 1]\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "min_perimeter (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert image array into image object\n",
    "get_image(im_array::Array) = colorview(Gray, im_array)\n",
    "# Find the suitable grid matrix\n",
    "function min_perimeter(total::Int64)\n",
    "    min_a = min_b = start = convert(Int64, ceil(sqrt(total)))\n",
    "    peri = Inf\n",
    "    for a in convert(Int64, floor(start/2)):start\n",
    "        for b in convert(Int64, floor(start/2)):start\n",
    "            if (a + b) * 2 < peri && total <= a * b\n",
    "                min_a, min_b = a, b\n",
    "                peri = (a + b) * 2\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return min_a, min_b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "display_image (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the images. `rows` is an array consisting the row number of the \n",
    "# images to display. The default is to randomly display 100 images.\n",
    "function display_image(rows::Array = [])\n",
    "    # Set up width and other variables\n",
    "    if rows == []\n",
    "        rows = rand(1:size(images, 1), 100)\n",
    "    end\n",
    "\n",
    "    if (tem_1 = sqrt(length(rows))) == (tem_2 = convert(Int64, ceil(tem_1)))\n",
    "        width = len = tem_2\n",
    "    else\n",
    "        len, width =  min_perimeter(length(rows))\n",
    "    end\n",
    "\n",
    "    fill_num = length(rows) - width * len             \n",
    "\n",
    "    # Preallocate a big square matrix\n",
    "    square = Array{Float64}(len * WIDTH, width * WIDTH)\n",
    "    row_cand = [i * WIDTH + 1 for i in 0:len-1]\n",
    "    col_cand = [i * WIDTH + 1 for i in 0:width-1]\n",
    "    i = 1\n",
    "    \n",
    "    # Moving small images into the big square in a loop\n",
    "    for row in row_cand\n",
    "        for col in col_cand\n",
    "            if i <= length(rows)\n",
    "                move_image!(row, col, square, restore(images[rows[i], :]))\n",
    "                i += 1\n",
    "            else\n",
    "                # Fill the extra black squares\n",
    "                move_image!(row, col, square, zeros(WIDTH, WIDTH))\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    get_image(square)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are some problems with Image.jl and Jupyter on Mac at this current branch.**\n",
    "\n",
    "Visualizing is not that important in this project, so I didnt do the optional problems and other visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 1.2 Model representation\n",
    "1. One array to represent the input layer(with a bias node).\n",
    "2. Two arrays for the hidden layer, one is for activated hidden layer(with a bias node).\n",
    "3. Two arrays for the output layer, one is for activated hidden layer.\n",
    "4. Two matrices for the weights between input and hidden layers, as well as between hidden and output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "using NLopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000×401 Array{Float64,2}:\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮                   ⋱  ⋮                        ⋮  \n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import weights data\n",
    "data = matread(\"data/ex4weights.mat\")\n",
    "pre_Θ1, pre_Θ2 = data[\"Theta1\"], data[\"Theta2\"]\n",
    "λ = 1\n",
    "ϵ = 0.12\n",
    "HU_NUM = 25\n",
    "DEBUG = -1 \n",
    "\n",
    "# Add value 1 column in the images matrix imported in `display.jl`\n",
    "feature = hcat([1 for i in 1:size(images, 1)], images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#=  Create output layer for this NN model, for this problem we would have 10\n",
    "    output units, since we are using the 1 of N encoding method. Each unit is\n",
    "    one entry, so we need to re-encode the output matrix. We put each output\n",
    "    as one 10 by 1 matrix, so the output matrix would be a 10 by 5000 matrix.\n",
    "=#\n",
    "function make_output_layer(digit::Float64)\n",
    "    tem = zeros(10)\n",
    "    tem[convert(Int64, digit)] = 1.0\n",
    "    tem\n",
    "end\n",
    "\n",
    "\n",
    "output = zeros(10)\n",
    "output[convert(Int64, numbers[1])] = 1.0\n",
    "for n in numbers[2:end]\n",
    "    output = hcat(output, make_output_layer(n))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Feedforward and cost function\n",
    "`Feedforward` function is implemented in the exercise 4. We only need to implement the cost function here.\n",
    "### Cost function\n",
    "The cost function is a generalized form of the cost function of logistic regression. Suppose we define the following variables:\n",
    "\n",
    "- $L$ = total number of layers in the network\n",
    "- $sl$ = number of units (not counting bias unit) in layer l\n",
    "- $K$ = number of output units/classes ($K \\geqslant 3$ or $K = 1$ because $K = 2$ is double class situation, we only need one output)\n",
    "\n",
    "\n",
    "Then the cost function is given as \n",
    "\n",
    "$$J(\\Theta) = - \\frac{1}{m} \\sum_{i=1}^m \\sum_{k=1}^K \\left[y^{(i)}_k \\log ((h_\\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\\log (1 - (h_\\Theta(x^{(i)}))_k)\\right] + \\frac{\\lambda}{2m}\\sum_{l=1}^{L-1} \\sum_{i=1}^{s_l} \\sum_{j=1}^{s_{l+1}} ( \\Theta_{j,i}^{(l)})^2$$\n",
    "\n",
    "- The first term is just adding an iteration for $K$\n",
    "- The regularization term is to give weights to all the parameters except those are the parameter of bias term. Those parameters are specialized as $\\Theta_{j,0}$, so we ignore $i = 0$. Since there is no input for the bias term in the next layer, so $j$ starts from $1$ as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict_test (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x can be either a number or an array\n",
    "function sigmoid(z)\n",
    "    if size(z) == ()\n",
    "        return 1 / (1 + e ^ (-z))\n",
    "    else\n",
    "        tem = Array{Float64}(length(z))\n",
    "        for i in 1:length(z)\n",
    "            tem[i] = 1 / (1 + e ^ (-z[i]))\n",
    "        end\n",
    "    end\n",
    "    return tem\n",
    "end\n",
    "\n",
    "# The gradient function of sigmoid function\n",
    "sigmoid_prime(z) = sigmoid(z) .* (1 - sigmoid(z))\n",
    "\n",
    "# Input layer unit x can be either a row vector or a column vector\n",
    "function h(Θ1, Θ2, x)\n",
    "    # x is the input layer, we want to get the hidden layer first\n",
    "    z2 = size(x,2) == 1 ? Θ1 * x : Θ1 * x'\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Get the output layer\n",
    "    # Add bias unit to a2\n",
    "    unshift!(a2, 1)\n",
    "    z3 = Θ2 * a2\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    return a3\n",
    "end\n",
    "\n",
    "# Input layer unit x can be either a row vector or a column vector\n",
    "function predict(Θ1, Θ2, x)\n",
    "    convert(Float64, find(p -> p == maximum(h(Θ1, Θ2, x)), h(Θ1, Θ2, x))[1])\n",
    "end\n",
    "\n",
    "# Test using result-known examples, return the correct rate. It also supports\n",
    "# returning the example which classifier fails to classify with the wrong \n",
    "# class prediction.\n",
    "function predict_test(feature, value, Θ1, Θ2)\n",
    "    correct_num = 0\n",
    "    failed = Dict{Int64, Int64}()\n",
    "    for t in 1:size(feature, 1)\n",
    "        pred =  predict(Θ1, Θ2, feature[t, :])\n",
    "        if  pred == value[t]\n",
    "            correct_num += 1\n",
    "        else\n",
    "            failed[t] = pred\n",
    "        end\n",
    "    end\n",
    "    return correct_num / size(feature, 1), failed\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cost (generic function with 2 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cost function for this NN model\n",
    "function cost(Θ1, Θ2, debug_num::Int64 = -1)\n",
    "    # Computing the first term of the cost function\n",
    "    first_term = 0.0\n",
    "    m = debug_num == -1 ? size(feature, 1) : debug_num\n",
    "    for i in 1:m\n",
    "        p = h(Θ1, Θ2, feature[i,:])\n",
    "        first_term = first_term + (-1) * output[:,i]' * \n",
    "                     log(p) - (1 - output[:,i])' * log(1 - p)\n",
    "    end\n",
    "    first_term /= m\n",
    "    \n",
    "    # Computing the second term of the cost function (regularization term)\n",
    "    # We don't regulate the bias weights, so we skip the first column of each\n",
    "    # weight matrix\n",
    "    second_term = λ / (2 * m) * (sum(Θ1[:, 2:end].^2) + sum(Θ2[:, 2:end].^2))\n",
    "\n",
    "    return (first_term + second_term)[1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38376985909092337"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(pre_Θ1, pre_Θ2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Backpropagation\n",
    "## 2.1 Sigmoid gradient\n",
    "Backpropagation requres the gradient function for activation function, and we know from the lecture that the gradient for sigmoid function is pretty easy to compute. I did not explicitly implemented a function for it. Instead, we use the formula directly in the backpropagation process.\n",
    "\n",
    "## 2.2 Random initialization\n",
    "If we set all weights to the same number, then all weights connecting to one output node will always be the same. So we need to random initialization. There are some research on how to initialize weights, here we use the one on the course page.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_weights (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using random initialization to initialize the weights\n",
    "# Return tuple (Θ1, Θ2)\n",
    "function init_weights()\n",
    "    Θ1 = Array{Float64,2}(HU_NUM, size(feature, 2))\n",
    "    Θ2 = Array{Float64,2}(size(output, 1), HU_NUM + 1)\n",
    "\n",
    "    # Init Θ1\n",
    "    for i in 1:size(Θ1, 1)\n",
    "        for j in 1:size(Θ1, 2)\n",
    "            Θ1[i, j] = rand(collect(-1 * ϵ : 0.0001 : ϵ))\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Init Θ2\n",
    "    for i in 1:size(Θ2, 1)\n",
    "        for j in 1:size(Θ2, 2)\n",
    "            Θ2[i, j] = rand(collect(-1 * ϵ : 0.0001 : ϵ))\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return (Θ1, Θ2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Backpropagation\n",
    "We are interested in the error term (partial derivative of cost function in terms of $z$ ,$\\delta_j^{(l)} = \\dfrac{\\partial}{\\partial z_j^{(l)}} cost(t)%]$) for each layer. Then we have:\n",
    "- $\\delta^{(L)} = a^{(L)} - y^{(t)}$ (the output layer)\n",
    "- $\\delta^{(l)} = ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .\\times \\  g^{\\prime}(z^{(l)}) =  ((\\Theta^{(l)})^T \\delta^{(l+1)})\\ .\\times \\ [a^{(l)}\\ .\\times \\ (1 - a^{(l)})]$ (for the hidden layers)\n",
    "- The order goes from the output layer backwards\n",
    "- If there is no regularization term, then $\\frac \\partial {\\partial \\Theta_{ij}^{(l)}} J(\\Theta) = a^{(l)}_{j} + \\delta^{(l)}_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 2 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=  Backward propagation: we first use forward propagation to compute the \n",
    "    hidden units, then use backward propagation to update the weights based on\n",
    "    the known error for each weights.\n",
    "=#\n",
    "function backward(Θ1, Θ2, debug_num::Int64 = -1)\n",
    "    # Initialize the accumulator\n",
    "    Δ1 = zeros(size(Θ1))\n",
    "    Δ2 = zeros(size(Θ2))\n",
    "    \n",
    "    # Iterate through the feature space\n",
    "    m = debug_num == -1 ? size(feature, 1) : debug_num\n",
    "    for i in 1:m\n",
    "        # Step 1: Forward computing\n",
    "        a1 = feature[i, :]\n",
    "\n",
    "        # x is the input layer, we want to get the hidden layer first\n",
    "        z2 = Θ1 * a1\n",
    "        a2 = sigmoid(z2)\n",
    "\n",
    "        # Get the output layer\n",
    "        # Add bias unit to a2\n",
    "        unshift!(a2, 1)\n",
    "        z3 = Θ2 * a2\n",
    "        a3 = sigmoid(z3)\n",
    "\n",
    "        # Step 2: Get the output error\n",
    "        δ3 = a3 - output[:, i]\n",
    "\n",
    "        # Step 3: Get the hidden weights error\n",
    "        # I didn't use the gradient function here because I want the bias unit\n",
    "        # to be added. Then it is more convenient to remove it in the next step.\n",
    "        δ2 = Θ2' * δ3 .* (a2 .* (1 - a2))\n",
    "\n",
    "        # Step 4: Accumulate those δ into Δ\n",
    "        Δ2 = Δ2 + δ3 * a2'\n",
    "        Δ1 = Δ1 + δ2[2:end] * a1'\n",
    "\n",
    "    end\n",
    "\n",
    "    # Then we add regularization\n",
    "    D2 = hcat((Δ2[:, 1] ./ m), (Δ2[:, 2:end]./ m + (λ / m) .* Θ2[:, 2:end]))\n",
    "    D1 = hcat((Δ1[:, 1] ./ m), (Δ1[:, 2:end]./ m + (λ / m) .* Θ1[:, 2:end]))\n",
    "\n",
    "    return (D1, D2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rolling\n",
    "To use optimizing packages, we want to roll the matrcies into a vector. So I made some functions to roll matrices, and unroll(restore) matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unrolling (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rolling two matrix into one vector\n",
    "rolling(Θ1, Θ2) = [Θ1[:]; Θ2[:]]\n",
    "\n",
    "# Unrolling one vector into two matrices by given dimension\n",
    "# The first matrix is i1 by j1 and the second one is i2 by j2\n",
    "function unrolling(roll, i1, j1, i2, j2)\n",
    "   return (reshape(roll[1:i1 * j1], i1, j1), \n",
    "           reshape(roll[i1 * j1 + 1:end], i2, j2))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization configue\n",
    "We need to follow some \"grammer\" for different optimizaiton packages, so we need to wrap around the backward function and cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cost_optim (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need to modify the gradient function a little bit to use Optim.jl\n",
    "function g!(x::Vector, storage::Vector)\n",
    "    # First unroll the parameters\n",
    "    (Θ1, Θ2) = unrolling(x, HU_NUM, size(feature, 2), \n",
    "                           size(output, 1), HU_NUM + 1)\n",
    "\n",
    "    # Get the gradient for those parameters\n",
    "    (D1, D2) = backward(Θ1, Θ2, DEBUG)\n",
    "\n",
    "    # Rolling the gradients and add to storage\n",
    "    storage[:] = rolling(D1, D2)\n",
    "end\n",
    "\n",
    "# Similarly, we want to modify the original cost function to use Optim.jl\n",
    "function cost_optim(x::Vector)\n",
    "    # First unroll the parameters\n",
    "    (Θ1, Θ2) = unrolling(x, HU_NUM, size(feature, 2), \n",
    "                           size(output, 1), HU_NUM + 1)\n",
    "\n",
    "    # Call the cost function\n",
    "    return cost(Θ1, Θ2, DEBUG)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Gradient checking\n",
    "To check the gradients we found from backward propagation are correct, we estimate the gradient using two states within a small difference $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "check_gradient (generic function with 2 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function check_gradient(para::Array{Float64,1}, untill::Int64 = -1)\n",
    "    ϵ = 1e-4\n",
    "    temp = zeros(size(para)...)\n",
    "    esti = zeros(size(para)...)\n",
    "    m = untill == -1 ? size(para, 1) : untill\n",
    "    # Iterate through the weights\n",
    "    for i in 1:m\n",
    "        print(\"$(i) in $(m) -> \")\n",
    "        temp[i] = ϵ\n",
    "        esti[i] = (cost_optim(para + temp) - cost_optim(para - temp)) / (2 * ϵ)\n",
    "        temp[i] = 0\n",
    "    end\n",
    "    \n",
    "    # Check the difference\n",
    "    current = Array{Float64, 1}(length(para))\n",
    "    g!(para, current)\n",
    "    diff = current[1:m] - esti[1:100]\n",
    "\n",
    "    println(\"The biggest difference among $(m) checkings is $(maximum(diff))\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can compare the estimated gradient with the gradient getting from backward propagation. We also notice that using this method to estimate gradient is very inefficient.\n",
    "\n",
    "Th biggest difference is extremly small (1.6824724231000177e-11), so we can conclude our backward function is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 in 100 -> 2 in 100 -> 3 in 100 -> 4 in 100 -> 5 in 100 -> 6 in 100 -> 7 in 100 -> 8 in 100 -> 9 in 100 -> 10 in 100 -> 11 in 100 -> 12 in 100 -> 13 in 100 -> 14 in 100 -> 15 in 100 -> 16 in 100 -> 17 in 100 -> 18 in 100 -> 19 in 100 -> 20 in 100 -> 21 in 100 -> 22 in 100 -> 23 in 100 -> 24 in 100 -> 25 in 100 -> 26 in 100 -> 27 in 100 -> 28 in 100 -> 29 in 100 -> 30 in 100 -> 31 in 100 -> 32 in 100 -> 33 in 100 -> 34 in 100 -> 35 in 100 -> 36 in 100 -> 37 in 100 -> 38 in 100 -> 39 in 100 -> 40 in 100 -> 41 in 100 -> 42 in 100 -> 43 in 100 -> 44 in 100 -> 45 in 100 -> 46 in 100 -> 47 in 100 -> 48 in 100 -> 49 in 100 -> 50 in 100 -> 51 in 100 -> 52 in 100 -> 53 in 100 -> 54 in 100 -> 55 in 100 -> 56 in 100 -> 57 in 100 -> 58 in 100 -> 59 in 100 -> 60 in 100 -> 61 in 100 -> 62 in 100 -> 63 in 100 -> 64 in 100 -> 65 in 100 -> 66 in 100 -> 67 in 100 -> 68 in 100 -> 69 in 100 -> 70 in 100 -> 71 in 100 -> 72 in 100 -> 73 in 100 -> 74 in 100 -> 75 in 100 -> 76 in 100 -> 77 in 100 -> 78 in 100 -> 79 in 100 -> 80 in 100 -> 81 in 100 -> 82 in 100 -> 83 in 100 -> 84 in 100 -> 85 in 100 -> 86 in 100 -> 87 in 100 -> 88 in 100 -> 89 in 100 -> 90 in 100 -> 91 in 100 -> 92 in 100 -> 93 in 100 -> 94 in 100 -> 95 in 100 -> 96 in 100 -> 97 in 100 -> 98 in 100 -> 99 in 100 -> 100 in 100 -> The biggest difference among 100 checkings is 1.6824724231000177e-11\n"
     ]
    }
   ],
   "source": [
    "check_gradient(rolling(pre_Θ1, pre_Θ2), 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Regularized Neural Networks\n",
    "We have already added the regularized terms into the backward function.\n",
    "\n",
    "## 2.6 Learning parameters using fmincg\n",
    "- I firstly used Optim.jl, but it was extremly slow. Then I changed to use NLopt.\n",
    "- Finally I found changing a different solver in Optim.jl can improve the accuracy.\n",
    "- We don't need it to converge, so set Epoch to a resonable range (I used 100 here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60.869896 seconds (49.95 M allocations: 97.449 GB, 17.78% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4941887500794949,[-0.623291,0.291996,-0.29824,-0.48469,0.144142,0.190506,0.120652,-1.20171,-0.196345,0.572185  …  -1.46556,-0.47885,0.362334,-1.33609,-1.06867,1.0646,0.697277,-0.227287,1.04276,-2.23368],:MAXEVAL_REACHED)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optim is so slow, let's try NLopt\n",
    "function costFunction!(x, storage)\n",
    "    cost = cost_optim(x)\n",
    "\n",
    "    # Store the gradient\n",
    "    if length(storage) > 0\n",
    "        g!(x, storage)\n",
    "    end\n",
    "    return cost\n",
    "end\n",
    "\n",
    "# Set up NLopt options\n",
    "initial = rolling(init_weights()...)\n",
    "opt = Opt(:LD_TNEWTON, length(initial))\n",
    "maxeval!(opt, 100)\n",
    "\n",
    "# Minimizing!\n",
    "min_objective!(opt, costFunction!)\n",
    "@time (minf,minx,ret) = NLopt.optimize(opt, initial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test the parameters we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9542,Dict(4991=>3,3437=>2,1113=>8,2109=>2,1564=>9,134=>4,4811=>4,2239=>9,1565=>2,4232=>2…))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unrolling the learned parameters\n",
    "mini_Θ1, mini_Θ2 = unrolling(minx, HU_NUM, size(feature, 2), \n",
    "                                 size(output, 1), HU_NUM + 1)\n",
    "\n",
    "# Get the accuracy and failed samples\n",
    "correct, failed = predict_test(feature, numbers, mini_Θ1, mini_Θ2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.95 on the training set is not bad."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
